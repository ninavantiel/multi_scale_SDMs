{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import wandb\n",
    "\n",
    "from GLC23PatchesProviders import MultipleRasterPatchProvider, RasterPatchProvider#, JpegPatchProvider\n",
    "from GLC23Datasets import PatchesDataset, PatchesDatasetMultiLabel\n",
    "from models import cnn\n",
    "from util import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NAME: 29_04_weighted_loss\n"
     ]
    }
   ],
   "source": [
    "run_name = '29_04_weighted_loss'\n",
    "print(f\"RUN NAME: {run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making patch providers for predictor variables...\n"
     ]
    }
   ],
   "source": [
    " # OCCURRENCE DATA\n",
    "data_path = 'data/full_data/'\n",
    "presence_only_path = data_path+'Presence_only_occurrences/Presences_only_train_sampled_100.csv'\n",
    "\n",
    "# COVARIATES\n",
    "print(\"Making patch providers for predictor variables...\")\n",
    "p_bioclim = MultipleRasterPatchProvider(data_path+'EnvironmentalRasters/Climate/BioClimatic_Average_1981-2010/') #19\n",
    "p_hfp_d = MultipleRasterPatchProvider(data_path+'EnvironmentalRasters/HumanFootprint/detailed/') #14\n",
    "p_hfp_s = RasterPatchProvider(data_path+'EnvironmentalRasters/HumanFootprint/summarized/HFP2009_WGS84.tif') #1\n",
    "    \n",
    "# presence only data = train dataset\n",
    "print(\"Making dataset for presence-only training data...\")\n",
    "presence_only_df = pd.read_csv(presence_only_path, sep=\";\", header='infer', low_memory=False)\n",
    "train_data = PatchesDatasetMultiLabel(\n",
    "    occurrences=presence_only_df.reset_index(), \n",
    "    providers=(p_bioclim, p_hfp_d, p_hfp_s)\n",
    ")\n",
    "print(f\"\\nTRAINING DATA: n={len(train_data)}\")\n",
    "\n",
    "# get number of features and number of species in train dataset\n",
    "n_features = train_data[0][0].cpu().detach().shape[0]\n",
    "print(f\"Number of covariates = {n_features}\")\n",
    "n_species = len(train_data.unique_sorted_targets)\n",
    "print(f\"Number of species = {n_species}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = cnn(n_features, n_species)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# load best model\n",
    "print(\"Loading best model checkpoint...\")\n",
    "checkpoint = torch.load(f\"models/{run_name}/best_val_loss.pth\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "print(f\"model checkpoint at epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"data/test_blind.csv\", sep=';')\n",
    "submission_data = PatchesDataset(\n",
    "    occurrences=submission,\n",
    "    id_name='Id',\n",
    "    label_name='Id',\n",
    "    providers=(p_bioclim, p_hfp_d, p_hfp_s),\n",
    "    ref_targets=train_data.unique_sorted_targets\n",
    ")   \n",
    "print(f\"SUBMISSION DATA: {len(submission_data)}\")\n",
    "submission_loader = torch.utils.data.DataLoader(submission_data, shuffle=False, batch_size=128, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "for inputs, labels in tqdm(submission_loader):\n",
    "    batch_y_pred = model(inputs)\n",
    "    y_pred_list.append(batch_y_pred.cpu().detach().numpy())\n",
    "\n",
    "y_pred = np.concatenate(y_pred_list)\n",
    "print(f\"y_pred shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# RUN NAME (for wandb and model directory)\n",
    "# run_name = '27_04_1320_full_data_sampled_100_weighted_loss'\n",
    "# run_name = '27_04_1435_full_data_sampled_100_unweighted_loss'\n",
    "\n",
    "\n",
    "thresholds = np.arange(0, 0.5, 0.05)\n",
    "\n",
    "# HYPERPARAMETER\n",
    "batch_size = 256\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(dev)\n",
    "\n",
    "    # OCCURRENCE DATA\n",
    "    data_path = 'data/full_data/'\n",
    "    # presence_only_path = data_path+'Presence_only_occurrences/Presences_only_train.csv'\n",
    "    presence_only_path = data_path+'Presence_only_occurrences/Presences_only_train_sampled_100.csv'\n",
    "    # presence_absence_path = data_path+'Presence_Absence_surveys/Presences_Absences_train.csv'\n",
    "    presence_absence_path = data_path+'Presence_Absence_surveys/Presences_Absences_train_sampled.csv'\n",
    "\n",
    "    # COVARIATES\n",
    "    print(\"Making patch providers for predictor variables...\")\n",
    "    p_bioclim = MultipleRasterPatchProvider(data_path+'EnvironmentalRasters/Climate/BioClimatic_Average_1981-2010/') #19\n",
    "    # p_elevation = RasterPatchProvider(data_path + 'EnvironmentalRasters/Elevation/ASTER_Elevation.tif') #1\n",
    "    p_hfp_d = MultipleRasterPatchProvider(data_path+'EnvironmentalRasters/HumanFootprint/detailed/') #14\n",
    "    p_hfp_s = RasterPatchProvider(data_path+'EnvironmentalRasters/HumanFootprint/summarized/HFP2009_WGS84.tif') #1\n",
    "    # p_rgb = JpegPatchProvider(data_path+'SatelliteImages/')#, dataset_stats='jpeg_patches_sample_stats.csv') # take all sentinel imagery layer (4)\n",
    "    # p_soil = MultipleRasterPatchProvider(data_path+'EnvironmentalRasters/Soilgrids/') #9\n",
    "    \n",
    "    # presence only data = train dataset\n",
    "    print(\"Making dataset for presence-only training data...\")\n",
    "    presence_only_df = pd.read_csv(presence_only_path, sep=\";\", header='infer', low_memory=False)\n",
    "    train_data = PatchesDatasetMultiLabel(\n",
    "        occurrences=presence_only_df.reset_index(), \n",
    "        providers=(p_bioclim, p_hfp_d, p_hfp_s)\n",
    "    )\n",
    "    print(f\"\\nTRAINING DATA: n={len(train_data)}\")\n",
    "\n",
    "    # get number of features and number of species in train dataset\n",
    "    n_features = train_data[0][0].cpu().detach().shape[0]\n",
    "    print(f\"Number of covariates = {n_features}\")\n",
    "    n_species = len(train_data.unique_sorted_targets)\n",
    "    print(f\"Number of species = {n_species}\")\n",
    "\n",
    "    # presence absence data = validation dataset\n",
    "    print(\"Making dataset for presence-absence validation data...\")\n",
    "    presence_absence_df = pd.read_csv(presence_absence_path, sep=\";\", header='infer', low_memory=False)\n",
    "    val_data = PatchesDatasetMultiLabel(\n",
    "        occurrences=presence_absence_df, \n",
    "        providers=(p_bioclim, p_hfp_d, p_hfp_s),\n",
    "        ref_targets=train_data.unique_sorted_targets\n",
    "    )\n",
    "    print(f\"TEST DATA: n={len(val_data)}\")\n",
    "\n",
    "    # data loaders\n",
    "    # train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=16)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, shuffle=False, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "\n",
    "\n",
    "    if os.path.exists(f\"models/{run_name}/y_pred_epoch_{str(checkpoint['epoch'])}.npy\"):\n",
    "        print(\"Loading y_pred and y_true...\")\n",
    "        y_pred =  np.load(f\"models/{run_name}/y_pred_epoch_{str(checkpoint['epoch'])}.npy\")\n",
    "        y_true = np.load(f\"models/{run_name}/y_true.npy\")\n",
    "    else:\n",
    "        print(\"Computing y_pred and y_true...\")\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            y_true_list.append(labels)\n",
    "            batch_y_pred = model(inputs.to(dev))\n",
    "            y_pred_list.append(batch_y_pred.cpu().detach().numpy())\n",
    "\n",
    "        y_pred = np.concatenate(y_pred_list)\n",
    "        print(f\"y_pred shape: {y_pred.shape}\")\n",
    "        np.save(f\"models/{run_name}/y_pred_epoch_{str(epoch)}.npy\", y_pred)\n",
    "        y_true = np.concatenate(y_true_list)\n",
    "        print(f\"y_true shape: {y_true.shape}\")\n",
    "        np.save(f\"models/{run_name}/y_true.npy\", y_true)\n",
    "\n",
    "    #thresholds = np.arange(0, 1, 0.1)\n",
    "    f1_scores = []\n",
    "    for thresh in tqdm(thresholds):\n",
    "        y_bin = np.where(y_pred > thresh, 1, 0)\n",
    "        f1_scores.append(np.array([f1_score(y_true[i,:], y_bin[i,:]) for i in range(y_true.shape[0])]).mean())\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    best_f1 = np.max(f1_scores)      \n",
    "    print(f\"Thresholds: {thresholds}\\nF1-scores: {f1_scores}\")\n",
    "    print(f\"Best threshold={best_threshold} --> validation F1-score={best_f1}\")\n",
    "    #np.save(f\"models/{run_name}/f1_scores_epoch_{str(epoch)}\", np.vstack(thresholds, f1_scores))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glc23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
