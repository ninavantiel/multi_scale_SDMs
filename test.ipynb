{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.PatchesProviders import RasterPatchProvider, MultipleRasterPatchProvider\n",
    "from data.Datasets import PatchesDataset, PatchesDatasetCooccurrences\n",
    "from models import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'data/full_data/'\n",
    "bioclim_dir = datadir+'EnvironmentalRasters/Climate/BioClimatic_Average_1981-2010/'\n",
    "soil_dir = datadir+'EnvironmentalRasters/Soilgrids/'\n",
    "po_path = datadir+'Presence_only_occurrences/Presences_only_train_sampled_10_percent_min_100_occurrences.csv' #Presences_only_train_sampled_25_percent_min_10_occurrences.csv'\n",
    "pa_path = datadir+'Presence_Absence_surveys/Presences_Absences_train.csv'\n",
    "\n",
    "pa_val_path = datadir+'Presence_Absence_surveys/Presences_Absences_train_80_percent.csv'\n",
    "pa_test_path = datadir+'Presence_Absence_surveys/Presences_Absences_train_20_percent.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1\n",
    "flatten = True\n",
    "p_bioclim = MultipleRasterPatchProvider(bioclim_dir, size=patch_size, flatten=flatten) \n",
    "p_soil = MultipleRasterPatchProvider(soil_dir, size=patch_size, flatten=flatten) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PatchesDataset(occurrences=po_path, providers=(p_bioclim, p_soil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data.species), train_data.species[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name='speciesId'\n",
    "item_columns=['lat','lon','patchID','dayOfYear']\n",
    "df = pd.read_csv(po_path, sep=\";\", header='infer', low_memory=False)\n",
    "species = train_data.species\n",
    "items = df[item_columns + [label_name]]\n",
    "items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items2 = pd.DataFrame(df.groupby(item_columns)[label_name].agg(list)).reset_index()\n",
    "items2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby(item_columns)[label_name].count()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[counts > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index in range(items2.shape[0]):\n",
    "    item = items2.iloc[index][item_columns].to_dict()\n",
    "    item_sps = items2.iloc[index][label_name]\n",
    "    labels = 1 * np.isin(species, item_sps)\n",
    "    if np.sum(labels) == 0:\n",
    "        print('!!!!!!!!', index)\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "item = items2.iloc[index][item_columns].to_dict()\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sps = items2.iloc[index][label_name]\n",
    "print(len(item_sps), item_sps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 1 * np.isin(species, item_sps)\n",
    "labels_f = 1 * np.isin(species, item_sps_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(labels == labels_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(labels), np.sum(labels_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[~items['speciesId'].isin(species)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 447\n",
    "item = items.iloc[index][item_columns].to_dict()\n",
    "item_species = items.query(\n",
    "    ' and '.join([f'{k} == {v}' for k, v in item.items()])\n",
    ")[label_name].values\n",
    "labels = 1 * np.isin(species, item_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(s, s in species) for s in item_species]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.sum([s in species for s in item_species])}/{len(item_species)}\")\n",
    "print(np.sum(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Making dataset for presence-only training data...\")\n",
    "train_data = PatchesDataset(occurrences=po_path, providers=(p_bioclim, p_soil))\n",
    "print(f\"\\nTRAINING DATA: n_items={len(train_data)}, n_species={len(train_data.species)}\")\n",
    "\n",
    "n_features = train_data[0][0].shape[0]\n",
    "n_species = len(train_data.species)\n",
    "print(f\"nb of features = {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Making dataset for presence-absence validation data...\")\n",
    "val_data = PatchesDataset(occurrences=pa_path, providers=(p_bioclim, p_soil), species=train_data.species)\n",
    "print(f\"\\nVALIDATION DATA: n_items={len(val_data)}, n_species={len(val_data.species)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "batch_size = 1024\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=16)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, shuffle=False, batch_size=batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = '0116_MLP_env_2x2_flat_train_tinyPO'\n",
    "checkpoint = torch.load(f\"models/{run_name}/best_val_loss.pth\")\n",
    "checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 5\n",
    "width = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = MLP(n_features, n_species, n_layers, width)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_loss_list, labels_list, y_pred_list = [], [], []\n",
    "\n",
    "for inputs, labels in tqdm(val_loader):\n",
    "    inputs = inputs.to(torch.float32)\n",
    "    labels = labels.to(torch.float32)\n",
    "    labels_list.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    y_pred = model(inputs)\n",
    "    y_pred_sigmoid = torch.sigmoid(y_pred)\n",
    "    y_pred_list.append(y_pred_sigmoid.cpu().detach().numpy())\n",
    "\n",
    "    val_loss = loss_fn(y_pred, labels)\n",
    "    val_loss_list.append(val_loss.cpu().detach())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate(labels_list)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate(y_pred_list)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(labels, y_pred)\n",
    "\n",
    "# patch size = 1            AUC = 0.5725362624834227\n",
    "# patch size = 2 (flat)     AUC = 0.46973261663879734"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glc23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
